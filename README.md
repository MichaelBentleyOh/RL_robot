개요
---
1. 스터디 목적(오민식)
2. 강화학습 기본 이론(이연수)
3. DRL 설명 => DQN(최원강), TD3(오민식)
4.
5. 5. project 주제 설명=> Pick and Place(최원강)
6. requiremnets(최원강)
7. ref code process architecture(옥윤정)
8. ref code 설명(이연수)
9. 시행착오들(image2action, camera2world coordinate, etc...)(옥윤정)
10. 최종 결과(오민식)
---
스터디 목적
---
스터디 기간 : 24.03.15 ~ 24.12.23

본 스터디는 심층 신경망 강화학습(Deep Reinforcement Learning : DRL) 공부를 통해 구성한 스터디로, 기초부터 시작하여, 실제 로봇에 적용하는 것으로 목표로 하였다.

결론적으로는 실제 로봇 적용에는 실패하였지만, DRL에 대한 이론 및 동작 방식을 이해할 수 있었다.



---
## 2. 강화학습 기본 이론

**강화학습(Reinforcement Learning, RL)** 은 **에이전트(agent)** 가 **환경(environment)** 과 상호작용하며 보상을 최대화하기 위한 **최적의 정책(optimal policy)** 을 학습하는 과정입니다. 에이전트는 반복적으로 **상태(state)** 를 관찰하고 **행동(action)** 을 선택하며 환경에서 반환된 **보상(reward)** 을 통해 학습합니다.


### 2.1 강화학습의 주요 요소

강화학습은 다음과 같은 주요 요소들로 구성됩니다.

#### 1. **환경 (Environment)**  
- 에이전트가 상호작용하는 공간이며, 행동의 결과로 상태를 업데이트하고 보상을 반환합니다.  
- 환경은 시뮬레이션 공간, 게임 환경, 실제 세계 등으로 구성될 수 있습니다.  
- 예: 로봇 시뮬레이션 환경, 체스 게임 보드

#### 2. **상태 (State)**  
- 현재 환경의 상태를 나타내는 정보의 집합
- 상태는 에이전트의 행동을 결정하는 데 사용됩니다.  
- 예: 로봇 팔의 현재 각도와 위치, 게임 내 캐릭터의 좌표 

#### 3. **행동 (Action)**  
- 에이전트가 특정 상태에서 수행할 수 있는 동작
- 행동은 이산적(discrete)일 수도 있고 연속적(continuous)일 수도 있습니다.  
- 예: 로봇 팔을 특정 축으로 회전시키기, 게임에서 이동 방향 선택

#### 4. **보상 (Reward)**  
- 에이전트가 특정 행동을 수행한 결과로 환경에서 반환받는 값 
- 보상은 행동이 목표 달성에 얼마나 기여했는지 피드백으로 제공됩니다.  
- 예: 로봇이 목표 물체를 성공적으로 집으면 +1, 실패하면 -1

#### 5. **정책 (Policy)**  
- 에이전트가 주어진 상태에서 어떤 행동을 선택할지 결정하는 규칙 또는 함수
- 정책은 확률적으로 행동을 선택할 수도 있고, 항상 특정 행동을 선택하도록 설계될 수도 있습니다.  
- 강화학습의 목표는 **최적의 정책**을 학습하는 것입니다.  

#### 6. **가치 함수 (Value Function)**  
- 특정 상태에서 에이전트가 받을 수 있는 **장기적인 보상의 기대값**을 나타냅니다.  
- 가치 함수는 상태 자체의 가치를 평가하거나, 상태와 행동 쌍의 가치를 평가하는 데 사용됩니다.


### 2.2 강화학습의 주요 학습 방법
강화학습은 학습 방식에 따라 **가치 기반 학습(Value-Based Learning)**, **정책 기반 학습(Policy-Based Learning)**, 그리고 이 두 가지를 결합한 **하이브리드 학습(Hybrid Learning)** 으로 나뉩니다.

#### 1. **가치 기반 학습 (Value-Based Learning)**

가치 기반 학습은 **상태-행동 쌍의 가치를 나타내는 함수(Q-값)** 를 학습하여 최적의 행동을 선택합니다.  
이 방식에서는 정책이 정의되며 에이전트는 Q-값이 가장 높은 행동을 선택합니다.

- **특징:**
  - 상태-행동 쌍의 가치를 예측하는 Q-함수 학습
  - Q-값을 최대화하는 행동을 선택

- **대표 알고리즘:**
  - **Q-러닝:** 상태와 행동의 Q-값을 테이블 형태로 업데이트
  - **DQN (Deep Q-Network):** Q-러닝을 심층 신경망으로 확장하여 고차원 상태 공간에 적용 가능

- **장점:**
  - 이산적인 행동 공간에 적합
  - 정책을 명시적으로 저장할 필요가 없음

- **단점:**
  - 연속적인 행동 공간에서는 적용이 어려움


#### 2. **정책 기반 학습 (Policy-Based Learning)**

정책 기반 학습은 최적의 정책을 **직접 학습**하여 에이전트가 행동을 선택할 수 있도록 합니다.  
특히, 연속적인 행동 공간에서 유리합니다.

- **특징:**
  - 상태에 따라 행동을 선택하는 정책 함수(Policy Function)를 직접 학습
  - 확률적 정책(확률 분포를 사용하여 행동 선택) 또는 결정적 정책(항상 특정 행동 선택)이 가능

- **대표 알고리즘:**
  - **REINFORCE:** 정책 경사법을 사용하여 보상을 최대화하도록 정책을 학습
  - **DDPG (Deep Deterministic Policy Gradient):** 연속적인 행동 공간에서 작동하는 결정적 정책 학습 알고리즘

- **장점:**
  - 연속적인 행동 공간에 적합
  - 탐색(exploration)과 활용(exploitation)을 적절하게 사용

- **단점:**
  - 보상이 희박하거나 변동성이 클 때 학습이 불안정



#### 3. **하이브리드 학습 (Hybrid Learning)**

하이브리드 학습은 가치 기반 학습과 정책 기반 학습의 장점을 결합하여 두 방법의 단점을 보완합니다.  
주로 가치 함수와 정책을 동시에 학습하여 보다 효율적인 학습을 가능하게 합니다.

- **특징:**
  - 정책과 가치 함수가 서로를 보완하며 학습
  - 가치 기반 방법은 정책 학습을 위한 추가적인 정보를 제공

- **대표 알고리즘:**
  - **A3C (Asynchronous Advantage Actor-Critic):** 멀티스레드를 활용해 효율적인 정책 및 가치 학습
  - **TD3 (Twin Delayed Deep Deterministic Policy Gradient):** DDPG의 개선 알고리즘으로, 연속 행동 공간에서 정책 학습의 안정성을 향상

- **장점:**
  - 복잡한 환경에서 더 안정적이고 효과적
  - 가치 함수의 피드백으로 정책의 학습 속도를 개선

- **단점:**
  - 구현이 비교적 복잡
  - 학습 과정에서 높은 계산 자원 요구
 

### 2.3 강화학습의 학습 과정
강화학습은 에이전트가 환경과 상호작용하며 다음 단계를 반복하는 과정으로 이루어집니다:

#### 1. **환경 초기화**  
   - 환경을 초기 상태로 설정하고, 에이전트는 첫 번째 상태를 관찰합니다.

#### 2. **행동 선택**  
   - 에이전트는 현재 상태에서 정책에 따라 행동을 선택합니다.  
   - 탐색(Exploration)과 활용(Exploitation)의 균형을 유지합니다.

#### 3. **환경 상호작용**  
   - 에이전트의 행동에 따라 환경은 새로운 상태와 보상을 반환합니다.

#### 4. **정책 및 가치 업데이트**  
   - 에이전트는 경험 데이터를 기반으로 정책과 가치 함수를 업데이트합니다.

#### 5. **반복**  
   - 목표를 달성하거나 종료 조건을 만족할 때까지 위 단계를 반복합니다.

---
DQN (Deep Q-Network) 이론 및 구조 설명
DQN(Deep Q-Network)은 **강화 학습(Reinforcement Learning)**에서 Q-러닝을 딥러닝으로 확장한 방법입니다. 

### 1. **Q-러닝의 한계**
Q-러닝은 상태-행동(State-Action) 쌍을 저장하는 **Q-테이블**을 사용합니다. 그러나:
- 상태 공간이 크거나 연속적이면 Q-테이블을 만들거나 갱신하는 데 **메모리와 계산 비용이 너무 큼**.
- 예를 들어, 이미지 기반 게임 같은 경우 수백만 개의 상태가 존재.
- 
### 2. **딥러닝을 활용한 Q 값 추정**
DQN은 딥러닝을 활용해 Q-러닝의 문제를 해결:
- **Q-함수**를 딥러닝 모델(주로 신경망)로 근사:
  - 상태를 입력으로 받아 행동별 Q 값을 출력.
  - 예: `Q(s, a)` 대신 `NN(s) -> [Q(s, a1), Q(s, a2), ..., Q(s, an)]`.
  - 
### 3. **DQN의 핵심 요소**
DQN은 다음 주요 기술로 구성됩니다:
#### (1) **리플레이 버퍼(Experience Replay)**:
- 학습 시 **샘플의 상관관계 제거**와 **데이터 효율성**을 높이기 위해 사용.
- 에이전트가 경험한 `(상태, 행동, 보상, 다음 상태)`를 저장.
- 무작위로 샘플링해 네트워크를 학습시켜 **데이터 중복 사용** 및 **편향 제거**.
- 
#### (2) **타겟 네트워크(Target Network)**:
- Q 값을 학습할 때 발생하는 **불안정성**을 줄이기 위해 도입.
- `Q(s, a)`의 타겟값(`r + γ * max_a' Q(s', a')`)을 안정적으로 계산하기 위해 **별도의 타겟 네트워크**를 사용:
  - 타겟 네트워크는 주기적으로 현재 네트워크의 가중치로 업데이트.
  - 이를 통해 학습 도중 발생하는 진동(oscillation)을 줄임.
  - 
#### (3) **탐험 vs 활용(Exploration vs Exploitation)**:
- **탐험(Exploration)**: 새로운 행동을 시도해 환경을 탐색.
- **활용(Exploitation)**: 현재 알고 있는 최적의 행동을 실행.
- DQN은 ε-greedy 정책을 사용:
  - 확률 ε로 랜덤 행동(탐험), 1-ε로 최적 행동(활용).
  - 학습이 진행될수록 ε 값을 줄여 점점 더 활용을 우선시.
  - 
#### (4) **손실 함수(Loss Function)**:
- 벨만 방정식을 기반으로 손실을 정의:
  - `Q(s, a; θ)`: 현재 네트워크에서 예측한 Q 값.
  - `r + γ * max_a' Q(s', a'; θ^-)`: 타겟 네트워크에서 계산한 실제 Q 값.
  - 이 차이를 최소화하도록 네트워크를 학습.
### 4. **DQN의 동작 흐름**
1. **환경 초기화**:
   - 초기 상태 S0 를 설정.
2. **행동 선택**:
   - ε-greedy 정책에 따라 행동 (A_t)를 선택.
   - 행동을 무작위(탐험)로 선택하거나 현재 Q 값 기반으로 선택(활용).
3. **환경 상호작용**:
   - 행동 (At)를 실행하고, 보상(Rt) 및 다음 상태 (St+1)를 수집.
4. **리플레이 버퍼 저장**:
   - (St, At, Rt, St+1)를 리플레이 버퍼에 저장.
5. **Q-네트워크 학습**:
   - 리플레이 버퍼에서 샘플을 추출해 벨만 방정식 기반 손실을 최소화하도록 네트워크 학습
6. **타겟 네트워크 업데이트**:
   - 주기적으로 타겟 네트워크 업데이트.
7. **반복**:
   - 목표 보상이 충분히 높아질 때까지 위 과정을 반복..
---

TD3 (Twin Delayed Deep Deterministic Policy Gradient) 이론 및 구조 설명
---
로봇 팔 Pick and Place 구현을 위해 TD3 (Twin Delayed Deep Deterministic Policy Gradient) 방법을 사용하려고 하였으나, 최종적으로는 실패하였다.

하지만 공부한 내용을 기록하기 위해 이론 설명을 추가하였으며, 참고한 논문에 대한 리뷰는 "ppt" 폴더에 정리되어 있다.

TD3는 강화학습 알고리즘 중 하나로, **DDPG (Deep Deterministic Policy Gradient)**의 한계를 극복하기 위해 제안된 방법이다. 

주로 연속적인 액션 공간에서 작동하며, 정책의 안정성과 학습 성능을 개선하는 데 중점을 둔다. 

TD3는 두 개의 주요 네트워크인 Actor Network와 Critic Network를 사용하며, 이를 통해 정책(Actor)과 가치 함수(Critic)를 학습한다.(그림 추가)

⚙️ 2. 구조와 구성 요소
---
A. Replay Buffer
경험 (s, a, r, s')을 저장한다. 경험(experience)은 agent가 environment와 상호작용한 정보를 의미하며, DRL(Deep Reinforcement Learning)은 경험을 training data로 활용한다.

목적: 데이터의 상관관계를 줄이고 학습을 안정화하기 위해 에이전트가 환경에서 수집한 경험을 replay buffer에서 무작위로 샘플링한다.

B. Critic Network
기능: 상태-액션 쌍 (s, a)에 대한 Q-값을 추정한다.

구성: 두 개의 독립적인 Q-함수 critic1, critic2와 이들의 타겟 네트워크 target1, target2로 구성된다.

TD Error Update (Temporal Difference Error): 두 Q-네트워크는 TD 에러를 기반으로 업데이트된다.

Target Q 값 비교: 두 Critic 네트워크에서 Q 값을 예측한 후, 더 작은 Q 값을 선택하여 overestimation bias를 줄인다.

Critic Network 학습 단계:

1. critic1과 critic2는 각각의 Q-값을 예측한다.

2. 타겟 네트워크(target1, target2)는 목표 Q-값을 계산한다. 두 Q-값 중 더 작은 값을 선택하여 학습한다.

C. Actor Network

기능: 주어진 상태 s에 대해 최적의 행동 a를 생성한다.

구성: actor와 target 두 네트워크로 구성된다.

DPG Update (Deterministic Policy Gradient): Actor Network는 Critic Network에서 전달받은 Q-값을 최대화하도록 업데이트된다.

Actor Network 학습 단계:

1. Actor Network는 주어진 상태 s에 대해 행동 μ(s)를 생성한다.

2. 행동에 노이즈 N를 추가하여 탐색을 수행한다.

3. Critic Network로부터 피드백을 받아 정책을 업데이트한다.

D. Environment

Actor Network에서 생성된 행동 a는 환경에 전달되고, 환경은 해당 행동을 받아들여 다음 상태 s'와 보상 r을 반환한다.

반환된 (s, a, r, s') 정보 experience는 Replay Buffer에 저장된다.

🔑 3. TD3의 핵심
---
Double Q-Learning

두 Critic Network를 사용하여 Q-값의 과대평가를 방지한다.
Target Policy Smoothing

Actor Network에 노이즈를 추가하여 급격한 정책 변화와 과적합을 방지한다.
Delayed Policy Update

Critic Network가 매 업데이트마다 학습되는 반면, Actor Network는 일정 주기마다 업데이트된다.
이로 인해 Critic Network가 더 안정적인 Q-값을 예측할 수 있다.

🛠️ 4. TD3 알고리즘 동작 순서
---
1) 초기화 단계

1-1) Actor와 Critic 네트워크 초기화

1-2) Replay Buffer 초기화

2) 경험 수집

2-1) Actor 네트워크를 통해 행동 a_t 선택

2-2) 환경에 행동 전달 및 보상과 다음 상태 (r_t, s_{t+1}) 반환

2-3) 경험 (s_t, a_t, r_t, s_{t+1})을 Replay Buffer에 저장

3) 샘플링 및 학습

3-1) Replay Buffer에서 무작위로 경험 샘플링

3-2) Critic 네트워크 업데이트 (TD 에러 기반)

3-3) Actor 네트워크는 Critic 네트워크 업데이트 주기마다 학습

4) 타겟 네트워크 업데이트

4-1) Critic 타겟과 Actor 타겟 네트워크는 지연된 소프트 업데이트 수행

5) 반복

5-1) 위 과정을 반복하여 정책과 Q-함수를 최적화

---

## 7. ref code process architecture
---
#### 1. 초기 설정 및 모델 로드
- 코드 실행 시 load_model() 함수가 호출되어, 사전 학습된 YOLO 객체 탐지 모델이 메모리에 로드.
- 로봇의 초기 상태는 reset_robot()을 통해 관절과 그리퍼를 초기 위치로 설정하며, 시뮬레이션 환경도 초기화.

#### 2. 객체 탐지
- 카메라 이미지 또는 주어진 입력 이미지를 detect() 함수로 처리.
- YOLO 모델을 사용하여 이미지 속에서 객체의 위치와 클래스 정보를 탐지하며, 바운딩 박스와 탐지된 클래스 리스트를 반환

#### 3. 중심 좌표 계산
- 탐지된 객체의 바운딩 박스 정보를 기반으로 calculate_centroid() 함수가 호출.
- 객체의 중심 좌표(이미지 상의 픽셀 위치)를 계산하여, 이후 로봇이 해당 위치를 목표로 삼을 수 있도록 함.

#### 4. 로봇 이동
- move_robot() 함수는 로봇의 관절 각도를 조절하여, 로봇의 끝단(End Effector)이 객체의 위치로 이동.
- 내부적으로 calculateIK()를 통해 목표 위치를 기반으로 역운동학(Inverse Kinematics)을 계산하고, 관절 제어 명령을 실행.

#### 5. 그리퍼 동작
- 로봇이 객체 근처에 도착하면 closeGripper()를 호출하여, 물체를 집을 수 있도록 그리퍼를 닫음.
- 객체를 이동한 뒤, openGripper()로 그리퍼를 열어 객체를 놓음.

#### 6. 객체 생성 및 배치
- 시뮬레이션 환경 내 객체는 create_parallelepiped(), create_l_object(), create_z_object()와 같은 함수로 생성.
- 다양한 형태의 객체를 특정 위치에 배치하며, 초기 상태를 설정.

#### 7. 시뮬레이션 업데이트
- 로봇의 움직임과 환경의 동작은 p.stepSimulation()을 통해 지속적으로 갱신되며, 모든 상태 변화가 물리적으로 계산.
- simulate_movement()는 목표 위치까지 로봇이 효율적으로 이동하도록 반복적인 계산과 시뮬레이션 단계를 처리

#### 8. 종료 및 반복
- 모든 작업이 완료된 후, 로봇과 객체의 상태는 초기화되거나 다음 작업 흐름을 위해 새로운 명령을 대기.
---
#### 5. 프로젝트 주제 설명**
### 주요 목표
1. **Pick-and-Place 작업**:
   - 로봇이 환경에서 물체를 탐지하고 정확한 위치로 이동하여 집어 올립니다.
   - 물체를 목표 위치에 배치하여 작업을 완료합니다.

2. **강화 학습 적용**:
   - 강화 학습 알고리즘(DQN)을 통해 로봇의 행동을 학습합니다.
   - 로봇은 시뮬레이션된 환경(PyBullet)에서 다양한 물체와 상호작용하며 학습.

3. **시뮬레이션 환경**:
   - PyBullet을 사용하여 현실적인 물리 환경을 모델링합니다.
   - 물체의 다양한 형태(L자형, Z자형 등)와 위치를 설정해 로봇이 다양한 작업을 수행하도록 합니다.

---

### 구성 요소
#### 1. **로봇 환경 (Environment)**:
   - `AmbienteRobot.py` 파일에서 정의된 `RobotEnv` 클래스는 로봇의 행동, 관찰, 보상 등을 설정합니다.
   - 관찰 공간:
     - 카메라 이미지(37x37 크기의 바이너리 이미지)를 통해 물체의 위치와 상태를 확인.
   - 행동 공간:
     - 로봇의 위치 조정 및 그리퍼(집게) 제어.
   - 보상 설계:
     - 물체를 성공적으로 들어 올리면 **+1**의 보상.
     - 실패 시 **-2**의 페널티.
     - 물체와 목표 위치 간의 거리에 따라 보상을 세분화 가능.

#### 2. **강화 학습 모델**:
   - DQN(Deep Q-Network)을 사용하여 로봇이 환경에서 최적의 행동을 학습.
   - `TrainEnvPandaPP.py`에서 모델 훈련 설정:
     - `CnnPolicy`: 이미지를 입력으로 받는 CNN 기반 정책 네트워크.
     - 학습률, 탐험 전략(ε-greedy), 배치 크기 등 최적화된 파라미터로 설정.
   - 훈련된 모델(`dqn_Z_6.zip`)을 테스트 시 사용.

#### 3. **물체의 형태와 환경 설정**:
   - 다양한 형태(L자형, Z자형 등)의 물체를 시뮬레이션.
   - 물체의 초기 위치와 방향을 무작위로 설정하여 환경 다양화.
   - 카메라 시점과 로봇의 행동을 조정해 현실적인 학습 환경 제공.

#### 4. **훈련과 테스트**:
   - 훈련:
     - 병렬 환경(`SubprocVecEnv`)을 사용하여 훈련 속도를 높임.
     - TensorBoard를 통해 학습 과정을 시각화.
   - 테스트:
     - `main.py`에서 훈련된 모델을 로드하여 테스트 환경에서 수행 성능 평가.

---
#### 6. 환경 설정 (Requirements)**
-Python: 3.8
-PyBullet: 3.2.5
-OpenCV (cv2): 4.9.0
-Stable-Baselines3: 2.4.0 (Python 3.8 지원의 마지막 버전)
-Gymnasium: 0.28.1
-NumPy: 2.2.0
-YOLO: V5
설치 코드
pip install -r requirements.txt
---
## 8. ref code 설명
`ref code`는 **PyBullet 기반의 로봇 강화학습 환경**을 구축하여 에이전트가 로봇 조작 작업을 학습할 수 있도록 설계되었습니다. 주요 구성 요소는 환경 초기화, 로봇 제어, 객체 생성 및 관찰 데이터 처리입니다. 아래는 코드의 주요 클래스와 메서드를 설명합니다.


### 1. **`RobotEnv` 클래스**

`RobotEnv` 클래스는 PyBullet 환경을 기반으로 강화학습에 적합한 시뮬레이션 환경을 제공합니다.

- **역할**  
  - 로봇과 객체 간의 상호작용을 구현  
  - 관찰값과 보상 데이터를 반환하여 강화학습 알고리즘에 전달  
  - 다양한 동작을 정의하여 에이전트가 학습 가능한 환경을 제공합니다.


### 2. **주요 메서드 설명**

#### 2.1 **환경 초기화**
- **`__init__()`**  
  - PyBullet 시뮬레이터를 초기화하며, GUI 또는 비GUI 모드로 실행됩니다.  
  - 로봇(Panda 로봇)과 다양한 객체를 로드하고 초기 상태를 설정합니다.  
  - 카메라를 구성하여 객체 중심의 관찰값을 생성하며 행동 및 관찰 공간을 정의합니다.

- **`reset()`**  
  - 새로운 에피소드를 시작하기 위해 환경을 초기화합니다.  
  - 객체의 초기 위치와 자세를 무작위로 생성하고 로봇의 관절 상태를 초기화합니다.  
  - 초기 관찰값과 추가 정보를 반환합니다.


#### 2.2 **로봇 동작**
- **`move_the_robot(target_position, angle)`**  
  - 로봇을 목표 위치로 이동시키고 물체를 조작하는 일련의 동작을 수행합니다.  
  - **이동 과정**  
    1. 로봇을 물체 상단으로 이동  
    2. 로봇을 물체 근처로 낮춤  
    3. 그리퍼를 닫아 물체를 집음  
    4. 물체를 목표 높이로 들어 올림  
  - `calculateIK()`를 통해 역운동학(Inverse Kinematics)을 계산하여 각 관절의 목표 각도를 결정합니다.  
  - 각 단계에서 PyBullet의 시뮬레이션 스텝(`p.stepSimulation()`)을 실행하여 로봇 동작을 업데이트합니다.

- **`closeGripper()` / `openGripper()`**  
  - 그리퍼를 닫거나 여는 동작을 수행합니다.  
  - 닫기 동작 중 특정 힘 이상의 저항이 감지되면 동작을 중단합니다.  
  - 그리퍼는 객체를 안정적으로 잡거나 놓을 수 있도록 설계되었습니다.


#### 2.3 **관찰 및 보상**
- **`get_observation()`**  
  - PyBullet 카메라를 통해 환경 이미지를 캡처하고, 객체 중심의 픽셀 블록으로 변환된 관찰값을 생성합니다.  
  - **이미지 처리 과정**  
    1. 객체의 윤곽선을 검출하여 이미지 상에서 위치를 식별  
    2. 윤곽선을 기준으로 객체의 중심 좌표를 계산  
    3. 관찰 이미지를 37x37 크기의 이진화된 픽셀 블록으로 변환  
  - 결과적으로 로봇이 관찰 가능한 객체 정보를 강화학습 알고리즘에 제공합니다

- **`calculate_reward()`**  
  - 객체가 목표 높이에 도달했는지를 기준으로 보상을 계산합니다.  
  - **보상 구조**  
    - 목표 높이에 도달 시 `+1`  
    - 실패하거나 목표 높이에 도달하지 못하면 `-2`  
  - 보상 설계는 거리 기반 보상 등으로 확장 가능


#### 2.4 **객체 생성 및 배치**
- **`create_parallelepiped()` / `create_l_object()` / `create_z_object()`**  
  - 다양한 형태의 객체를 시뮬레이션 환경에 생성합니다.  
  - 각 객체는 초기 위치와 자세를 무작위로 설정하며 URDF 형식을 통해 PyBullet에 로드됩니다.

- **`generate_new_pose()`**  
  - 객체의 초기 위치와 자세를 무작위로 생성합니다.  
  - 다양한 환경 설정을 통해 학습 데이터의 다양성을 높이고 에이전트의 일반화 성능을 강화합니다.


#### 2.5 **시뮬레이션 동작**
- **`step(action)`**  
  - 주어진 행동(action)에 따라 로봇의 동작을 실행하고 새로운 관찰값, 보상, 종료 여부를 반환합니다.  
  - 행동은 7x7 픽셀 블록과 여러 각도 조합으로 구성된 이산적 행동 공간에서 선택됩니다.  
  - 한 에피소드의 단계를 정의하며 강화학습 알고리즘에서 데이터를 학습하는 핵심 메서드입니다.

- **`simulate_movement()`**  
  - 로봇의 목표 위치까지의 움직임을 반복적으로 계산하고 물리적으로 시뮬레이션합니다.  
  - 각 시뮬레이션 단계에서 로봇 관절의 상태를 확인하며 목표에 도달했는지 판단합니다.


### 3. **보상 설계**
- 물체가 목표 높이에 도달하면 `+1`의 보상을 부여합니다.  
- 실패하거나 지정된 높이에 도달하지 못하면 `-2`의 패널티를 부여합니다.  
- 현재 보상 구조는 거리 기반 보상, 객체 회전 안정성 보상 등으로 확장 가능합니다.


---


## 9. 시행착오
---
### 1. camera2world coordinate
로봇이 카메라 영상을 바탕으로 큐브를 집기 위해서는 카메라의 픽셀 좌표를 월드 좌표로 변환하는 과정이 필요합니다. 카메라의 내부 파라미터는 공식을 통해 구하였으며, 회전 행렬(R)은 월드 좌표를 기준으로 y축을 +180°, z축을 -90° 회전한 것을 바탕으로 계산하였습니다. 변환 벡터(T)는 초기에는 링크 상태 간의 거리 차이를 이용하여 구했으나, 결과가 정확하지 않아 직접 계산을 통해 수정하였습니다. 그러나 큐브의 생성 위치가 달라질 때마다 변환 벡터가 달라지는 문제가 발생하였습니다. 이를 해결하기 위해 큐브의 생성 위치를 고정하고, 고정된 변환 벡터를 사용하여 작업을 시작하도록 수정하였습니다.

### 2. depth 정규화
큐브까지의 거리를 계산하기 위해 `arm_camera()`의 `depth_img`에서 depth 값을 추출하였으나, 해당 값이 0~255 로 정규화된 값임을 확인하였습니다. 이를 해결하기 위해 설정된 `far` 값을 활용하여 `depth = depth_value / 255.0 * far`로 변환하여 실제 depth값을 추출하였고, 문제를 해결하였습니다.

---
## 10. 결과
(추가 예정)
